from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.prompts import MessagesPlaceholder
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

import os
from dotenv import load_dotenv
import streamlit as st

# â”€â”€ æœ€åˆã® Streamlit ã‚³ãƒãƒ³ãƒ‰ â”€â”€
st.set_page_config(page_title="LLMã‚¢ãƒ—ãƒªï¼ˆæ•™ææå‡ºï¼‰", layout="centered")

# â”€â”€ APIã‚­ãƒ¼èª­ã¿è¾¼ã¿ï¼š.envï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰â†’ secretsï¼ˆCloudï¼‰ â”€â”€
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    try:
        if "OPENAI_API_KEY" in st.secrets and st.secrets["OPENAI_API_KEY"]:
            OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
    except Exception:
        pass
if not OPENAI_API_KEY:
    st.error(
        "OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.envï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰ã¾ãŸã¯ Streamlit Secretsï¼ˆCloudï¼‰ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# â”€â”€ LLM â”€â”€
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, timeout=60)

# â”€â”€ ãƒ­ãƒ¼ãƒ«å®šç¾©ï¼ˆ2æŠï¼‰â”€â”€
EXPERT_ROLES = {
    "å“è³ªä¿è¨¼ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ï¼ˆè£½é€ æ¥­ï¼‰": (
        "ã‚ãªãŸã¯è£½é€ æ¥­ã®å“è³ªä¿è¨¼ï¼ˆQAï¼‰ã¨çµ±è¨ˆçš„å“è³ªç®¡ç†ã®å°‚é–€å®¶ã§ã™ã€‚"
        "å·¥ç¨‹èƒ½åŠ›(Cp/Cpk)ã€ä¸è‰¯ãƒ¢ãƒ¼ãƒ‰ã€FMEAã€QCä¸ƒã¤é“å…·ã€PDCAã«ç²¾é€šã—ã€"
        "å…¥åŠ›å†…å®¹ã«å¯¾ã—ã¦æ ¹æ‹ ãƒ»æ‰‹é †ãƒ»æ•°å¼ãƒ»æ³¨æ„ç‚¹ãƒ»ç¾å®Ÿçš„åˆ¶ç´„ã‚’ä½µè¨˜ã—ã¤ã¤ã€"
        "å†ç¾å¯èƒ½ãªæ”¹å–„ç­–ã¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚"
    ),
    "å–¶æ¥­ã®ãƒ—ãƒ­ï¼ˆB2B/B2Cï¼‰": (
        "ã‚ãªãŸã¯B2B/B2Cã«ç²¾é€šã—ãŸå–¶æ¥­ã®ãƒ—ãƒ­ã§ã™ã€‚"
        "ãƒ’ã‚¢ãƒªãƒ³ã‚°è¨­è¨ˆã€èª²é¡Œæ·±æ˜ã‚Šï¼ˆSPINï¼‰ã€ä¾¡å€¤è¨´æ±‚ï¼ˆFABE/ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆï¼‰ã€"
        "å·®åˆ¥åŒ–ã€åè«–å‡¦ç†ã€æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆæ„ã€ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ã¾ã§ã‚’ä½“ç³»çš„ã«ææ¡ˆã—ã€"
        "ãƒˆãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ãƒ»è³ªå•ãƒªã‚¹ãƒˆãƒ»KPI/æŒ‡æ¨™ãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬ã‚’å…·ä½“çš„ã«æç¤ºã—ã¦ãã ã•ã„ã€‚"
        "é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚„æ„æ€æ±ºå®šè€…/ä½¿ç”¨è€…ã®åˆ‡ã‚Šåˆ†ã‘ã«ã‚‚è§¦ã‚Œã¦ãã ã•ã„ã€‚"
    ),
}

# â”€â”€ Promptï¼ˆå±¥æ­´ã‚’MessagesPlaceholderã§æ³¨å…¥ï¼‰â”€â”€
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "{system_message}"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input_text}"),
    ]
)
base_chain = prompt | llm

# â”€â”€ ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ â”€â”€
if "session_id" not in st.session_state:
    st.session_state.session_id = "default"
if "history_store" not in st.session_state:
    st.session_state.history_store = {}


def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    store = st.session_state.history_store
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]


# â”€â”€ RunnableWithMessageHistory â”€â”€
chain_with_history = RunnableWithMessageHistory(
    base_chain,
    get_session_history,
    input_messages_key="input_text",
    history_messages_key="history",
)

# â”€â”€ é€šå¸¸å‘¼ã³å‡ºã—ï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰â”€â”€


def ask_llm_once(input_text: str, role_key: str) -> str:
    system_message = EXPERT_ROLES.get(role_key, "")
    if not system_message:
        raise ValueError("ä¸æ˜ãªå°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ãŒé¸æŠã•ã‚Œã¾ã—ãŸã€‚")
    if not input_text or not input_text.strip():
        raise ValueError("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™ã€‚")

    result = chain_with_history.invoke(
        {"system_message": system_message, "input_text": input_text.strip()},
        config={"configurable": {"session_id": st.session_state.session_id}},
    )
    return result.content


# ================= UI =================
st.title(" LLMæ©Ÿèƒ½ä»˜ãWebã‚¢ãƒ—ãƒª")
with st.expander("ğŸ“˜ ã“ã®ã‚¢ãƒ—ãƒªã«ã¤ã„ã¦ï¼ˆæ¦‚è¦ãƒ»æ“ä½œæ–¹æ³•ï¼‰", expanded=True):
    st.markdown(
        """
**æ¦‚è¦**  
- Streamlit + LangChain ã® LLMã‚¢ãƒ—ãƒªã€‚  
- ãƒ©ã‚¸ã‚ªé¸æŠã®å°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ã«å¿œã˜ã¦ System ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ‡æ›¿ã€‚  
- **ä¼šè©±å±¥æ­´ã‚’è¸ã¾ãˆãŸå›ç­”**ã‚’ **é€šå¸¸å‡ºåŠ›**ã§ã¾ã¨ã‚ã¦è¡¨ç¤ºã—ã¾ã™ã€‚  

**æ“ä½œæ–¹æ³•**  
1. ãƒ­ãƒ¼ãƒ«ã‚’é¸æŠ  
2. ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›  
3. ã€Œé€ä¿¡ã€ã§å›ç­”è¡¨ç¤º  
4. ã€Œå±¥æ­´ã‚¯ãƒªã‚¢ã€ã§ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ  
        """
    )

role = st.radio("å°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", options=list(
    EXPERT_ROLES.keys()), horizontal=True)
user_text = st.text_area(
    "å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ",
    value="æ–°è¦ãƒ©ã‚¤ãƒ³ã®ä¸è‰¯ç‡ã‚’ä¸‹ã’ãŸã„ã€‚ç¾å ´ã§ä»Šæ—¥ã‹ã‚‰ã§ãã‚‹æ‰“ã¡æ‰‹ã‚’æ•™ãˆã¦ã€‚ï¼ˆä¾‹ï¼‰",
    height=140,
    placeholder="ã“ã“ã«ç›¸è«‡å†…å®¹ã¾ãŸã¯è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„â€¦",
)

cols = st.columns([1, 1, 2])
with cols[0]:
    submitted = st.button("é€ä¿¡", use_container_width=True)
with cols[1]:
    if st.button("å±¥æ­´ã‚¯ãƒªã‚¢", use_container_width=True):
        st.session_state.history_store.pop(st.session_state.session_id, None)
        st.success("å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
        st.rerun()

# å±¥æ­´ã®å¯è¦–åŒ–
with st.expander("ğŸ—‚ ä¼šè©±å±¥æ­´ï¼ˆã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰"):
    hist = get_session_history(st.session_state.session_id).messages
    if hist:
        for m in hist:
            who = "ğŸ‘¤ User" if m.type == "human" else "AI"
            st.markdown(f"- **{who}**: {m.content}")
    else:
        st.caption("ã¾ã å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

if submitted:
    try:
        with st.spinner("LLMã«å•ã„åˆã‚ã›ä¸­â€¦"):
            answer = ask_llm_once(user_text, role)
        st.success("å›ç­”ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        st.markdown("### âœ… å›ç­”")
        st.write(answer)
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š{e}")
